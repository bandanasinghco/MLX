{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba0902-845f-424b-8c58-2ec2be7fcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e133960-6f6a-492a-b56c-60ada4999d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \n",
    "    # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64, mask 200 x 200\n",
    "    \n",
    "    # For each of the heads, we will divide the q,k, and v value vector into 8 parts.\n",
    "    # So dividing the basically entire 1536 embedding size into 24 parts i.e 1536 \\ 24, so each q is 64\n",
    "    # now it concatenated each value of q,k,v, so it is 64 + 64 + 64 = 192\n",
    "\n",
    "    d_k = q.size()[-1] \n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
    "    print(f\"scaled.size() : {scaled.size()}\")\n",
    "    \n",
    "    if mask is not None:\n",
    "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
    "        scaled += mask # 30 x 8 x 200 x 200\n",
    "    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n",
    "    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9a24d-3a12-48d3-89b8-a67f6d48f6dd",
   "metadata": {},
   "source": [
    "we will get batch size * sequence_len * 64 and you will get up dimensional tensor. you will get 64 dime vector are now context aware because of attention, now this will be much more higher quality vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0555777-4b4e-4d5f-bfb4-90b703d6e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross attention:\n",
    "its going to take two arrows from encoder, and there is one arrow coming from decoder, from decoder that going to be the query vector and \n",
    "in encoder\n",
    "Thats going to be the key, the values vector from the encoder.\n",
    "\n",
    "we can see whats the dimensions of the tensor that occur and understanding why the dimension are the way they are.\n",
    "printing out what the exact output shape is\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd5886-d6cb-4c32-80c9-5a637affd8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
