{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c0b2de-8fda-4f29-a4ae-2766dc321754",
   "metadata": {},
   "source": [
    "**Project Overview**\n",
    "\n",
    "The core idea is to develop a system that can interpret and respond to user queries related to mental health. This will be achieved by fine-tuning the Mixtral 8x7B model, a Mixture of Experts (MoE) model known for its efficient handling of diverse tasks and large-scale data, on a targeted dataset from Hugging Face that includes mental health discussions or related content.\n",
    "\n",
    "**Dataset**: Utilizing a dataset specifically related to mental health, likely comprising textual data that includes questions, discussions, and information about mental health issues. This dataset is pivotal for training the model to understand and generate relevant responses within this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394647e0-3db2-4685-9b9e-82db61298bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('marmikpandya/mental-health')\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07134ecc-704c-4af0-838e-d1bfa1deaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a77a29-31ac-4539-8492-c3361a0d5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cf6fb-ea82-4356-8ad6-95513a357508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ad8d6-fd67-4986-ba93-aa7e88d08d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d649630-3b6e-4753-aac3-3b2bb3ffa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Or use df.fillna(method='ffill') to fill missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ab093-0db1-43db-ab95-5261874a8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "    # Example of removing irrelevant features\n",
    "    relevant_columns = ['input']  # Assuming 'text' is the relevant column\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # Handling missing data\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess the dataset\n",
    "df_clean = preprocess_data(dataset)\n",
    "print(\"Data preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3da414-156d-4c37-931f-4dd07b0d190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ed7f8-dbf3-4f7f-97c2-d6f00460e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9c92d-c7b4-42ac-b5d6-4934f8160d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer,DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc2961-011f-441f-9b55-5d24c5de8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "# Load the model with specific settings for 4-bit quantization and device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map=\"auto\"3 re\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer have been loaded with the specified configurations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9f3f0-5647-4dfd-9a97-b703c0ec804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install cudatoolkit=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34b055-3304-48e3-987b-824515f740a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a soft prompt to the tokenizer\n",
    "soft_prompt = \"The mental state described is: \"\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensuring padding is handled\n",
    "\n",
    "def add_soft_prompt(texts):\n",
    "    return [soft_prompt + text for text in texts]\n",
    "\n",
    "# Update dataset texts\n",
    "df_clean['text'] = add_soft_prompt(df_clean['text'].tolist())\n",
    "print(\"Soft prompts added to texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cc29-b28f-4073-9beb-323e8ea5f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning the Model\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=4,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    push_to_hub=False,               # Whether to push model to the hub\n",
    "    fp16=True,                       # Use mixed precision training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=df_clean,\n",
    "    eval_dataset=df_clean,  # Ideally, you should have a separate validation set\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print(\"Model fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398724e-5228-4c54-a81e-7606ebc33b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training, evaluate your model to see how well it performs:\n",
    "\n",
    "eval_results = trainer.evaluate(\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "\n",
    "# Save the model for deployment\n",
    "model.save_pretrained('./mental_health_chatbot_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a1ca-9ba9-4952-bd07-8274a2039aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860dd49-bafb-4134-b7f6-f76b20d38d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = './mental_health_chatbot_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mixtral-8x7b')  # Assuming you used this tokenizer\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14c2e-06ad-4034-9089-deb3616361f2",
   "metadata": {},
   "source": [
    "Step 2: Create a Function to Generate Responses\n",
    "You will need a function that takes user input, tokenizes it, passes it through the model, and then decodes the modelâ€™s output into a human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bd018-6752-4067-b0ee-e702e9e4dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move input tensors to the same device as model\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    # Generate output logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities (softmax)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Assuming a binary classification (e.g., 0: No issue, 1: Issue detected)\n",
    "    response_label = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0, response_label].item()\n",
    "\n",
    "    # Define responses based on the detected label\n",
    "    if response_label == 1:\n",
    "        return f\"There seems to be a concern. I highly recommend seeking professional help. (Confidence: {confidence:.2f})\"\n",
    "    else:\n",
    "        return f\"Everything seems normal. If you feel this isn't right, please consult a professional. (Confidence: {confidence:.2f})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753dcb4-ca76-433b-a6e1-a1b278b68043",
   "metadata": {},
   "source": [
    "setting up a Simple Command-Line Interface\n",
    "This allows for real-time interaction with the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73f1e3-1f5e-4b67-9fe0-572918efb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Hello! I'm your Mental Health Assistant. How can I assist you today?\\n\")\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in ['exit', 'quit']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            response = generate_response(user_input)\n",
    "            print(\"Bot:\", response)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccf5b6-ca11-4b67-8d91-2e3f33a3134d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657dfea-8065-4bfd-bf8c-0438614a80dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
