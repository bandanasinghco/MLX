{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47797cf-fc9c-48bc-9913-14ade33a3040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01059ecb-a600-494a-a199-e7013ecd93a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bfa8f-befd-48cc-a6f0-9c5ff919d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c44a8533-e668-4d01-9a4a-f93d06526941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119714</th>\n",
       "      <td>Once upon a time, in a small town, there lived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119715</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119716</th>\n",
       "      <td>Once upon a time, there was a big tree. Under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119717</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119718</th>\n",
       "      <td>Once upon a time, there was an adorable little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2119719 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        One day, a little girl named Lily found a need...\n",
       "1        Once upon a time, there was a little car named...\n",
       "2        One day, a little fish named Fin was swimming ...\n",
       "3        Once upon a time, in a land full of trees, the...\n",
       "4        Once upon a time, there was a little girl name...\n",
       "...                                                    ...\n",
       "2119714  Once upon a time, in a small town, there lived...\n",
       "2119715  Once upon a time, there was a little boy named...\n",
       "2119716  Once upon a time, there was a big tree. Under ...\n",
       "2119717  Once upon a time, there was a little girl name...\n",
       "2119718  Once upon a time, there was an adorable little...\n",
       "\n",
       "[2119719 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11b4f1-eeec-4d30-bc2c-69ba61887b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59615235-ba56-474c-a8d8-94c7b7f46509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from transformers import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a820ed86-4c94-4adb-9cdd-108a7d7e1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, max_length=128):\n",
    "        self.stories = stories\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        story = self.stories[idx]['text']  # Access the 'text' value of the story dictionary\n",
    "        input_ids = self.tokenizer(\"<sos>\" + story, max_length=self.max_length, padding='max_length', truncation=True)['input_ids']\n",
    "        label_ids = self.tokenizer(story + \"</sos>\", max_length=self.max_length, padding='max_length', truncation=True)['input_ids']\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90439cda-1d55-4fe1-b024-ab451d463ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset from Hugging Face\n",
    "dataset = load_dataset('roneneldan/TinyStories')\n",
    "\n",
    "# Split the dataset into training and validation splits\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Tokenize the stories using a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the story datasets\n",
    "train_story_dataset = StoryDataset(train_dataset, tokenizer)\n",
    "val_story_dataset = StoryDataset(val_dataset, tokenizer)\n",
    "\n",
    "# Create the data loaders\n",
    "train_dataloader = DataLoader(train_story_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_story_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dac2bf4d-4289-4bf4-964c-6eb620c69e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1c92053a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "946fe50e-68db-4e49-b167-4a3049b536db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1edd8d82-115c-428b-aa8d-d8d72b505a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.positional_encoding = self.generate_positional_encoding()\n",
    "        \n",
    "    def generate_positional_encoding(self):\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        position = torch.arange(0, self.max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2613101e-b1c3-4904-8360-527d87d51101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, head_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.head_dim = d_model // self.num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        Reshape to (batch_size, num_heads, seq_len, head_dim)\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        weighted_sum = torch.matmul(attention, value)\n",
    "        \n",
    "        weighted_sum = weighted_sum.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.fc(weighted_sum)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65412d90-4514-4d2b-9556-2847da9356b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, d_model // num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self_attention_output = self.self_attention(x, x, x)\n",
    "        self_attention_output = self.norm1(x + self_attention_output)\n",
    "        \n",
    "        feed_forward_output = self.feed_forward(self_attention_output)\n",
    "        output = self.norm3(self_attention_output + feed_forward_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76001028-ad84-40ad-993a-41f890041175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82ba5bb5-16f9-4f4b-ac4b-c92bd39e7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the Transformer model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 100\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 128   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4b90eba-8fa6-495e-9663-253334bd2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7413a3f4-6761-4724-a2c7-5d10deb06046",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size, d_model, num_layers, num_heads, d_ff)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bed307ec-c4ca-47f4-aa3a-2c386d841ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for epoch in range(num_epochs):\\n    total_loss = 0\\n    \\n    for batch in train_dataloader:\\n        input_ids, label_ids = batch\\n        \\n        # Forward pass\\n        output = model(input_ids)\\n        \\n        # Reshape the output and label_ids to match the loss function requirements\\n        output = output.view(-1, vocab_size)\\n        label_ids = label_ids.view(-1)\\n        \\n        # Calculate the loss\\n        loss = criterion(output, label_ids)\\n        \\n        # Backward pass and optimization\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n        total_loss += loss.item()\\n    \\n    # Print the average loss for the epoch\\n    avg_loss = total_loss / len(train_dataloader)\\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\\n    '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids, label_ids = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_ids)\n",
    "        \n",
    "        # Reshape the output and label_ids to match the loss function requirements\n",
    "        output = output.view(-1, vocab_size)\n",
    "        label_ids = label_ids.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label_ids)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67614aa-26d4-4380-82bc-64dcf3f8285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a1e620d-5ab1-4a92-86c7-b5a1d601fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "641d47ff-8f4c-4305-87bb-b78e1dd59433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                | 0/1059860 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, -1, 8, 12]' is invalid for input of size 25600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, :max_divisor]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Reshape the output and label_ids to match the loss function requirements\u001b[39;00m\n\u001b[1;32m     34\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 43\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     self_attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     self_attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m self_attention_output)\n\u001b[1;32m     25\u001b[0m     feed_forward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(self_attention_output)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(key)\n\u001b[1;32m     19\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(value)\n\u001b[0;32m---> 21\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     22\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, -1, 8, 12]' is invalid for input of size 25600"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # Assume that num_heads is 8\n",
    " \n",
    "    \n",
    "    # Wrap your dataloader with tqdm for a progress bar\n",
    "    train_dataloader_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    for batch in train_dataloader_progress:\n",
    "        # Define batch_size\n",
    "        batch_size = 32  \n",
    "        \n",
    "        input_ids, label_ids = batch\n",
    "        num_heads = 8\n",
    "        \n",
    "\n",
    "         # Make sure that the sequence length is divisible by num_heads\n",
    "        sequence_length = input_ids.shape[1] # This should be divisible by num_heads\n",
    "            \n",
    "            \n",
    "        # Check if the sequence length is divisible by num_heads\n",
    "        if sequence_length % num_heads != 0:\n",
    "            # If not, trim the input_ids tensor to the largest divisor of sequence_length\n",
    "            max_divisor = sequence_length - (sequence_length % num_heads)\n",
    "            input_ids = input_ids[:, :max_divisor]\n",
    "\n",
    "    \n",
    "        # Forward pass\n",
    "        output = model(input_ids)\n",
    "        \n",
    "        # Reshape the output and label_ids to match the loss function requirements\n",
    "        output = output.view(-1, vocab_size)\n",
    "        label_ids = label_ids.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label_ids)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar\n",
    "        train_dataloader_progress.set_postfix({'loss': total_loss / (batch_idx+1)})\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00b60500-f6f7-4636-9a56-4247ef482fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming the input size to the MultiHeadAttention layer is (batch_size, seq_len, embed_size)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# And the `query` tensor is of shape (batch_size, seq_len, embed_size)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the total number of elements in the `query` tensor\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m total_elements \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mnelement()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the expected number of elements after reshaping\u001b[39;00m\n\u001b[1;32m      8\u001b[0m expected_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "# input size to the MultiHeadAttention layer is (batch_size, seq_len, embed_size)\n",
    "\n",
    "# Calculate the total number of elements in the `query` tensor\n",
    "total_elements = query.nelement()\n",
    "\n",
    "# Calculate the expected number of elements after reshaping\n",
    "expected_elements = 2 * self.num_heads * self.head_dim * -1\n",
    "\n",
    "# Check if the total number of elements matches the expected number of elements\n",
    "assert total_elements == expected_elements, f\"Number of elements does not match. Expected {expected_elements}, but got {total_elements}.\"\n",
    "\n",
    "# If the assertion passes, proceed with the reshaping operation\n",
    "query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a02dce-1cee-4fa2-9c34-c1e94b191f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936d4f4-c032-45ce-84f0-b6654586da0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0ccdc-5d98-4bb7-9eb7-1d504285732e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b9d0f-1a3f-4eb6-8d6b-ac946a451850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere(self, starting_word, max_length, sp_model):\n",
    "        device = next(self.parameters()).device  # Get the device of the model\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        hidden = self.init_hidden(1)  # Assume batch size of 1 for inference\n",
    "        \n",
    "        # Convert the starting word to its index in the vocabulary\n",
    "        #starting_index = vocab[starting_word]\n",
    "        starting_index = sp_model.encode_as_ids(starting_word)\n",
    "        print(starting_index)\n",
    "        # Initialize the input sequence with the starting word\n",
    "        input_token = torch.tensor([[starting_index]]).to(device)\n",
    "\n",
    "        generated_text = [starting_word]\n",
    "\n",
    "        # Loop until the end token is generated or max length is reached\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through the decoder\n",
    "            output, predicted_ids, hidden = self.forward(input_token, hidden, '')\n",
    "\n",
    "            # Get the predicted token index (assuming batch size of 1)\n",
    "            predicted_index = predicted_ids.item()\n",
    "\n",
    "            # Convert the predicted token index to its corresponding word\n",
    "            predicted_word = sp_model.decode(predicted_index)\n",
    "\n",
    "            # Append the predicted word to the generated text\n",
    "            generated_text.append(predicted_word)\n",
    "\n",
    "            # Check if the end token is generated\n",
    "            if predicted_word == '<end>':\n",
    "                break\n",
    "\n",
    "            # Prepare the input for the next iteration\n",
    "            input_token = torch.tensor([[predicted_index]]).to(device)\n",
    "\n",
    "        # Combine the generated words into a single string\n",
    "        generated_text = ' '.join(generated_text)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cd701-0c06-4e3e-9116-4a79d120ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary\n",
    "vocab_list = [sp_model.id_to_piece(i) for i in range(vocab_size)]\n",
    "# Create a vocabulary dictionary mapping words to indices\n",
    "vocab = {idx: word for idx, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b1f1b-5bc7-4b36-b78e-8419b4bb46cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
