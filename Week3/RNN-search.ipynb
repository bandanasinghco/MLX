{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8893b36-e5f3-42e9-bc9c-43ae2dcb8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Dataset from MS MARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b7b44-08f1-460b-a858-443797104960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Adjust the dataset name/version as necessary\n",
    "dataset = load_dataset(\"ms_marco\", \"v2.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4391d3-aa77-4019-8725-790c923c9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into Training, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d689127b-2a67-4e32-bc95-78875cb5bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "dataset_splits = {\n",
    "    \"train\": train_testvalid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "    \"valid\": test_valid[\"train\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be822286-7b8a-40c6-a152-03c3080de4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Queries and Documents\n",
    "    #You'll iterate through the data to extract queries and their corresponding relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4595b06-e211-421b-aeb7-25663414d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Triples of Queries, Relevant and Irrelevant Documents\n",
    "    #This step involves negative sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5331974-79c6-4150-884e-e84e336f0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_triples(queries, docs, num_negatives=10):\n",
    "    triples = []\n",
    "    all_docs = [doc for sublist in docs for doc in sublist]  # Flatten list of docs\n",
    "    for i, query in enumerate(queries):\n",
    "        relevant_docs = docs[i]\n",
    "        for rel_doc in relevant_docs:\n",
    "            # Sample irrelevant documents\n",
    "            for _ in range(num_negatives):\n",
    "                irr_doc = random.choice(all_docs)\n",
    "                while irr_doc in relevant_docs:\n",
    "                    irr_doc = random.choice(all_docs)\n",
    "                triples.append((query, rel_doc, irr_doc))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123b86f3-7411-4727-a8a6-47e5898eafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Your Generated Data\n",
    "# For tokenization, you'll use SentencePiece to generate tokens and then map these tokens using word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff442d-5061-4e99-9153-0b3d4f15632b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509ba96c-6d9c-4983-acc7-663d9d332e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HAVING ERROR WHILE SAVING FILE. It seems like because of the dataset structure.\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def create_text_file_from_dataset(dataset, filename=\"ms_marco_texts.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for item in dataset:\n",
    "            # Write the 'query' to the file if it exists (assuming 'query' is direct key in each item)\n",
    "            if 'query' in item:\n",
    "                text_file.write(item['query'] + \"\\n\")\n",
    "            \n",
    "            # Now directly iterate over 'passage_text' since it's a list of strings, not a list of dictionaries\n",
    "            if 'passage_text' in item:\n",
    "                for passage_text in item['passage_text']:\n",
    "                    text_file.write(passage_text + \"\\n\")\n",
    "\n",
    "# Call this function with the subset of data\n",
    "create_text_file_from_dataset(subset, \"ms_marco_texts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cfee525-7bcb-4275-bdab-66214128320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=ms_marco_texts.txt --model_prefix=m --vocab_size=100\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ms_marco_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ms_marco_texts.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 100 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=3634\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9725% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=38\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999725\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 100 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1597\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 769 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 100\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 379\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 379 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=488 obj=13.1839 num_tokens=1026 num_tokens/piece=2.10246\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=433 obj=12.5953 num_tokens=1031 num_tokens/piece=2.38106\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=324 obj=12.914 num_tokens=1137 num_tokens/piece=3.50926\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=324 obj=12.5811 num_tokens=1137 num_tokens/piece=3.50926\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=243"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " obj=13.3801 num_tokens=1277 num_tokens/piece=5.25514\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=243 obj=13.0262 num_tokens=1277 num_tokens/piece=5.25514\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=182 obj=14.0565 num_tokens=1462 num_tokens/piece=8.03297\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=182 obj=13.6725 num_tokens=1463 num_tokens/piece=8.03846\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=136 obj=14.6286 num_tokens=1642 num_tokens/piece=12.0735\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=136 obj=14.3438 num_tokens=1642 num_tokens/piece=12.0735\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=110 obj=14.7929 num_tokens=1770 num_tokens/piece=16.0909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=110 obj=14.5905 num_tokens=1772 num_tokens/piece=16.1091\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=ms_marco_texts.txt --model_prefix=m --vocab_size=100')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32c7b7f8-ea09-493e-b1b9-3594f2b6f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_triples(triples):\n",
    "    tokenized_triples = []\n",
    "    for query, rel_doc, irr_doc in triples:\n",
    "        tokenized_query = sp.encode_as_pieces(query)\n",
    "        tokenized_rel_doc = sp.encode_as_pieces(rel_doc)\n",
    "        tokenized_irr_doc = sp.encode_as_pieces(irr_doc)\n",
    "        tokenized_triples.append((tokenized_query, tokenized_rel_doc, tokenized_irr_doc))\n",
    "    return tokenized_triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c6923db-f8d8-451f-b5d2-c81f87981eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For word2vec embeddings, you'll typically load a pre-trained model and map each token to its embedding. If you're using Gensim's word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9e75440-e8d2-4122-8a9a-778b62e7c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")  \n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model to use later\n",
    "model.wv.save_word2vec_format('word2vec_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f29e213e-d529-4e5b-96ea-dbc8b96b2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec_model.bin', binary=True)\n",
    "\n",
    "def embed_tokens(tokenized_text):\n",
    "    embeddings = []\n",
    "    for token in tokenized_text:\n",
    "        if token in word_vectors:\n",
    "            embeddings.append(word_vectors[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(word_vectors.vector_size)) \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c084880-73e7-4637-bdcd-62ac99314563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Two-Tower architecuture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b40801a0-a28c-4c80-aeed-4ce12364f5e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m query_encoding, pos_doc_encoding, neg_doc_encoding\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Assume we have loaded the pretrained word2vec embeddings into a variable named `pretrained_embedding`\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Let's also assume we have the embedding_dim and hidden_dim set, along with num_layers for the RNN\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m TwoTowerModel(pretrained_embedding\u001b[38;5;241m=\u001b[39m\u001b[43mpretrained_embedding\u001b[49m,\n\u001b[1;32m     40\u001b[0m                       embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     41\u001b[0m                       hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[1;32m     42\u001b[0m                       num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, embedding_dim, hidden_dim, num_layers):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer with pre-trained weights.\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)  # You can set freeze=False to fine-tune\n",
    "        \n",
    "        # RNN encoder for queries.\n",
    "        self.query_rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # RNN encoder for documents.\n",
    "        self.doc_rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, query_tokens, pos_doc_tokens, neg_doc_tokens):\n",
    "        # Embed tokens\n",
    "        query_embeds = self.embedding(query_tokens)\n",
    "        pos_doc_embeds = self.embedding(pos_doc_tokens)\n",
    "        neg_doc_embeds = self.embedding(neg_doc_tokens)\n",
    "        \n",
    "        # Encode using RNN\n",
    "        _, query_encoding = self.query_rnn(query_embeds)\n",
    "        _, pos_doc_encoding = self.doc_rnn(pos_doc_embeds)\n",
    "        _, neg_doc_encoding = self.doc_rnn(neg_doc_embeds)\n",
    "        \n",
    "        # We use only the last hidden state for encoding\n",
    "        query_encoding = query_encoding[-1]\n",
    "        pos_doc_encoding = pos_doc_encoding[-1]\n",
    "        neg_doc_encoding = neg_doc_encoding[-1]\n",
    "        \n",
    "        return query_encoding, pos_doc_encoding, neg_doc_encoding\n",
    "\n",
    "# Assume we have loaded the pretrained word2vec embeddings into a variable named `pretrained_embedding`\n",
    "# Let's also assume we have the embedding_dim and hidden_dim set, along with num_layers for the RNN\n",
    "\n",
    "model = TwoTowerModel(pretrained_embedding=pretrained_embedding,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ecbe5-e94e-4933-983d-aaeb0e6b9a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef414805-2b8c-461d-89eb-298f2830a67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71c0e3b7-8f57-4622-8265-8506456a86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the model's forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e9e2e-2d8e-41a7-8930-8d04a74a0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, query_tokens, pos_doc_tokens, neg_doc_tokens):\n",
    "        # Embed tokens\n",
    "        query_embeds = self.embedding(query_tokens)\n",
    "        pos_doc_embeds = self.embedding(pos_doc_tokens)\n",
    "        neg_doc_embeds = self.embedding(neg_doc_tokens)\n",
    "        \n",
    "        # Pass embeddings through the RNN encoder\n",
    "        _, query_rnn_output = self.query_rnn(query_embeds)\n",
    "        _, pos_doc_rnn_output = self.doc_rnn(pos_doc_embeds)\n",
    "        _, neg_doc_rnn_output = self.doc_rnn(neg_doc_embeds)\n",
    "        \n",
    "        # Extract the last hidden states as the encodings\n",
    "        query_encoding = query_rnn_output[-1]\n",
    "        pos_doc_encoding = pos_doc_rnn_output[-1]\n",
    "        neg_doc_encoding = neg_doc_rnn_output[-1]\n",
    "        \n",
    "        return query_encoding, pos_doc_encoding, neg_doc_encoding\n",
    "\n",
    "# Initialize the Triplet Loss\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "#  tensors of token indices with shape (batch_size, sequence_length)\n",
    "query_tokens = torch.tensor([[...], [...], ...])\n",
    "pos_doc_tokens = torch.tensor([[...], [...], ...])\n",
    "neg_doc_tokens = torch.tensor([[...], [...], ...])\n",
    "\n",
    "# Forward pass\n",
    "query_encoding, pos_doc_encoding, neg_doc_encoding = model(query_tokens, pos_doc_tokens, neg_doc_tokens)\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(query_encoding, pos_doc_encoding, neg_doc_encoding)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceeb398-9905-4855-a5e4-00994e09e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, query_encoder, doc_encoder):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.query_encoder = query_encoder\n",
    "        self.doc_encoder = doc_encoder\n",
    "    \n",
    "    def forward(self, query_embeddings, doc_embeddings):\n",
    "        query_encodings = self.query_encoder(query_embeddings)\n",
    "        doc_encodings = self.doc_encoder(doc_embeddings)\n",
    "        return query_encodings, doc_encodings\n",
    "    \n",
    "    def compute_distance(self, query_encodings, doc_encodings):\n",
    "        # Example distance function: Euclidean distance\n",
    "        distances = torch.norm(query_encodings - doc_encodings, dim=1)\n",
    "        return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c7943-c16a-429f-979b-e277e9b78fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, query_encodings, pos_doc_encodings, neg_doc_encodings):\n",
    "        # Compute distances\n",
    "        positive_distances = torch.norm(query_encodings - pos_doc_encodings, dim=1)\n",
    "        negative_distances = torch.norm(query_encodings - neg_doc_encodings, dim=1)\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        losses = torch.relu(positive_distances - negative_distances + self.margin)\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c478761-1561-44fb-8dc0-81aeee922b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embeddings, pos_doc_embeddings, and neg_doc_embeddings are your data tensors\n",
    "# query_encoder and doc_encoder are your initialized encoder models\n",
    "\n",
    "model = TwoTowerModel(query_encoder, doc_encoder)\n",
    "triplet_loss = TripletLoss(margin=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    query_encodings, pos_doc_encodings = model(query_embeddings, pos_doc_embeddings)\n",
    "    _, neg_doc_encodings = model(query_embeddings, neg_doc_embeddings)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = triplet_loss(query_encodings, pos_doc_encodings, neg_doc_encodings)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbdd34fa-6c8e-43c5-b260-ed8589be0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-caching Document Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270bed2-2456-4b32-8079-b7ee1184f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_doc_encodings(doc_encoder, doc_embeddings):\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        doc_encodings = doc_encoder(doc_embeddings)\n",
    "    return doc_encodings\n",
    "\n",
    "# Assuming `doc_encoder` is your document encoder model and `doc_embeddings` is a tensor of your document embeddings\n",
    "doc_encodings = cache_doc_encodings(doc_encoder, doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff0a3aa6-43cc-4433-885e-760ad387d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Inference Function\n",
    "\n",
    "#For the inference function, we'll encode the incoming query, compute distances between the query encoding and all document encodings, and then select the top k documents with the smallest distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597e4ca-f24a-495b-a402-f2d2878da3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_documents(query_encoder, doc_encodings, query_embedding, k=5):\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        #Encode the query\n",
    "        query_encoding = query_encoder(query_embedding.unsqueeze(0))  # Add batch dimension\n",
    "        \n",
    "        # Compute distances or similarities\n",
    "        distances = torch.norm(doc_encodings - query_encoding, dim=1)  # Euclidean distance\n",
    "        # For cosine similarity, uncomment the following line\n",
    "        similarities = torch.nn.functional.cosine_similarity(doc_encodings, query_encoding, dim=1)\n",
    "        \n",
    "        # Find the top k smallest distances (or largest similarities)\n",
    "        top_k_indices = torch.topk(distances, k, largest=False).indices  # Change `largest=False` to `True` for similarities\n",
    "        # For similarities, uncomment the following line\n",
    "        top_k_indices = torch.topk(similarities, k, largest=True).indices\n",
    "        \n",
    "    return top_k_indices.tolist()\n",
    "\n",
    "# Assuming `query_encoder` is your query encoder model and `query_embedding` is the tensor of your query embedding\n",
    "top_k_indices = find_top_k_documents(query_encoder, doc_encodings, query_embedding, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901e8b1-67c9-4a5e-a9b6-bb0231bddc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Query Embedding and Retrieve Top \n",
    "'''To use 'find_top_k_documents',\n",
    "To use find_top_k_documents, you need to obtain the embedding for the new query'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc05498-9dd2-4de8-8a65-8a5cbfdee2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume `tokenizer` and `embeddings` are your preprocessing and embedding tools\n",
    "# query_text is the new query you received\n",
    "query_tokens = tokenizer(query_text)  # Tokenize the query text\n",
    "query_embedding = embeddings(query_tokens)  # Obtain embeddings for the query\n",
    "\n",
    "# Encode the query and find top k document indices\n",
    "top_k_indices = find_top_k_documents(query_encoder, doc_encodings, query_embedding, k=5)\n",
    "\n",
    "# Assuming `documents` is a list of your original documents\n",
    "top_k_documents = [documents[i] for i in top_k_indices]\n",
    "\n",
    "print(\"Top k documents:\", top_k_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4705bf8-610e-42ad-9823-632f65236505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c677a-cedb-48ed-8ce6-bd9f9f98be8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
