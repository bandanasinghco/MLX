{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522f42c0-e791-4d88-a2a4-5292e89716bc",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c783d9-5acc-4cbf-9840-4dae6d2e4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0db37-145b-4b7d-9ec3-591af6beb4ed",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107227b-93a6-4d04-9a09-9f417e200989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load marco data\n",
    "data = load_dataset('ms_marco', 'v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8da49-bdc0-4608-9dd8-c526a5851a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset = data['train']\n",
    "val_dataset = data['validation']\n",
    "test_dataset = data['test']\n",
    "\n",
    "# convert to pandas\n",
    "data_train_df = pd.DataFrame(train_dataset)\n",
    "data_validation_df = pd.DataFrame(val_dataset)\n",
    "data_test_df = pd.DataFrame(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df1356-e3b4-4bc0-b631-6b1492c509f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50406be-5b9a-4723-9a86-4eb6786d1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract queries and documents from the dataset\n",
    "queries = data_train_df['query'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f62b8-0803-4750-86fe-7829c72682aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = data_train_df['passages'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56b7e018-fb87-46de-af44-deaf8d53a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate triples of queries, relevant (positive) documents and irrelevant (negative) documents\n",
    "\n",
    "triples = []\n",
    "for i in range(len(queries)):\n",
    "    positive_document = relevant_documents[i]\n",
    "    negative_document = relevant_documents[(i+1)%len(relevant_documents)]  #  negative sampling\n",
    "    triples.append((queries[i], positive_document, negative_document))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5641b7-1419-4888-84d6-7f1355f619df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Tokenize the generated data using Sentencepiece\n",
    "# First, train the SentencePiece model\n",
    "vocab_size = 5000\n",
    "SentencePieceTrainer.Train(\n",
    "    input=triples,\n",
    "    model_prefix='m',\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=1.0,\n",
    "    model_type='unigram'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078225bb-d1a2-4181-8ed9-3ede92f06155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we can use the trained model to tokenize our data\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('m.model')\n",
    "\n",
    "tokenized_triples = [(sp.EncodeAsPieces(query), \n",
    "                      sp.EncodeAsPieces(pos_doc), \n",
    "                      sp.EncodeAsPieces(neg_doc)) for query, pos_doc, neg_doc in triples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432a9a-ad5a-4435-b1b6-7fb2e2f5a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tokenizing a query and document\n",
    "tokenized_query = sp.encode(train_triples[0][0], out_type=str)\n",
    "tokenized_positive_doc = sp.encode(train_triples[0][1], out_type=str)\n",
    "tokenized_negative_doc = sp.encode(train_triples[0][2], out_type=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
